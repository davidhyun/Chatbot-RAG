import os
from dotenv import load_dotenv
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

#Chroma tenant ì˜¤ë¥˜ ë°©ì§€ ìœ„í•œ ì½”ë“œ
import chromadb
chromadb.api.client.SharedSystemClient.clear_system_cache()

# Streamlitì€ ì‚¬ìš©ìì™€ ìƒí˜¸ì‘ìš©í•  ë•Œë§ˆë‹¤ ì „ì²´ íŒŒì´ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¬ì‹¤í–‰í•˜ì—¬ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ
# ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°ì½”ë ˆì´í„° '@st.cache_resource'ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ì•±ì´ êµ¬ë™ë  ë•Œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ìºì‹±í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©

# PDF íŒŒì¼ ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma(VectorDB) ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    split_docs = text_splitter.split_documents(_docs)
    persist_directory = "./chroma_db"
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=persist_directory
    )
    return vectorstore

# ê¸°ì¡´ì— ì €ì¥í•´ë‘” ChromaDBê°€ ìˆëŠ” ê²½ìš°, ì´ê²ƒì„ ë¡œë“œ
@st.cache_resource
def get_vector_store(_docs):
    persist_directory = "./chroma_db"
    if os.path.exists(persist_directory):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=OpenAIEmbeddings(model='text-embedding-3-small')
        )
    else:
        return create_vector_store(_docs)
    
# ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ëŠ” í—¬í¼ í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource
def chaining():
    # ê¸°ë°˜ ì§€ì‹(ë¬¸ì„œ) ì£¼ì…
    #file_path = r"data/ëŒ€í•œë¯¼êµ­í—Œë²•(í—Œë²•)(ì œ00010í˜¸)(19880225).pdf"
    file_path = "data/(á„Œá…¦2023-269á„’á…©)+á„‰á…¡á†«á„Œá…¥á†¼á„á…³á†¨á„…á…¨á„Œá…¦á„ƒá…©+á„Œá…µá†¯á„‹á…´á„‹á…³á†¼á„ƒá…¡á†¸.pdf"
    
    # ë¬¸ì„œ ë¶„í•     
    pages = load_and_split_pdf(file_path)
    
    # ë¬¸ì„œë¥¼ ìˆ˜ì¹˜í™”(ì„ë² ë”©)í•˜ì—¬ ë²¡í„° DBì— ì €ì¥
    vectorstore = get_vector_store(pages)

    # ë²¡í„° DBë¥¼ ê²€ìƒ‰ê¸°ë¡œ ì„ ì–¸
    retriever = vectorstore.as_retriever()
    
    qa_system_prompt = """
        You are an assistant for question-answering tasks. \
        Use the following pieces of retrieved context to answer the question. \
        If you don't know the answer, just say that you don't know. \
        Keep the answer perfect. please use imogi with the answer.
        Please answer in Korean and use respectful language.\
        {context}
    """
    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )
    
    # ì–¸ì–´ ëª¨ë¸ ì„ ì–¸
    llm = ChatOpenAI(model='gpt-4o')
    
    # RAG ì²´ì¸
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain


# Streamlit UI
st.title("ì‚°ì •íŠ¹ë¡€ Q&A ì±—ë´‡ ğŸ’¬")

rag_chain = chaining()

# session_stateì— messages Key ê°’ ì§€ì • ë° Streamlit í™”ë©´ ì§„ì… ì‹œ, AIì˜ ì¸ì‚¬ë§ì„ ê¸°ë¡í•˜ê¸°
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”. ì‚°ì •íŠ¹ë¡€ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"            
        }
    ]
    
# ì‚¬ìš©ìë‚˜ AIê°€ ì§ˆë¬¸/ë‹µë³€ ì£¼ê³  ë°›ì„ ì‹œ, ì´ë¥¼ ê¸°ë¡í•˜ëŠ” session_state
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :"):
    st.chat_message("human").write(prompt_message)
    st.session_state.messages.append({"role": "user", "content": prompt_message})
    with st.chat_message("ai"):
        with st.spinner("ë‘ë‡Œ í’€ê°€ë™ ì¤‘..."):
            response = rag_chain.invoke(prompt_message)
            st.session_state.messages.append({"role": "assistant", "content": response})
            st.write(response)
    
# ì±—ë´‡ìœ¼ë¡œ í™œìš©í•  AI ëª¨ë¸ ì„ ì–¸
chat = ChatOpenAI(model="gpt-4o", temperature=0)

# chat_input()ì— ì…ë ¥ê°’ì´ ìˆëŠ” ê²½ìš°
if prompt := st.chat_input():
    # messagesë¼ëŠ” session_stateì— ì—­í• ì€ ì‚¬ìš©ì, ì½˜í…ì¸ ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ê°ê° ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # chat_message() í•¨ìˆ˜ë¡œ ì‚¬ìš©ì ì±„íŒ… ë²„ë¸”ì— Prompt ë©”ì‹œì§€ë¥¼ ê¸°ë¡
    st.chat_message("user").write(prompt)
    
    response = chat.invoke(prompt)
    msg = response.content
    
    # messagesë¼ëŠ” session_stateì— ì—­í• ì€ AI, ì½˜í…ì¸ ëŠ” API ë‹µë³€ì„ ê°ê° ì €ì¥
    st.session_state.messages.append({"role": "assistant", "content": msg})
    
    # chat_message() í•¨ìˆ˜ë¡œ AI ì±„íŒ… ë²„ë¸”ì— API ë‹µë³€ì„ ê¸°ë¡
    st.chat_message("assistant").write(msg)