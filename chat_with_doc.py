from dotenv import load_dotenv
import streamlit as st
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

load_dotenv()
    
# Streamlitì—ì„œ ì—…ë¡œë“œí•˜ë©´ íŒŒì¼ ìì²´ë¥¼ ì €ì¥í•˜ê¸° ë•Œë¬¸ì— ì„ì‹œ íŒŒì¼ì— ë‚´ìš©ì„ ì €ì¥í•˜ê³  íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë°˜í™˜í•˜ëŠ” ìš°íšŒ ë°©ë²•ì„ ì‚¬ìš©
@st.cache_resource
def load_pdf(_file):
    # ì„ì‹œ íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ì—…ë¡œë“œëœ PDF íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ì €ì¥
    with tempfile.NamedTemporaryFile(mode="wb", delete=False) as tmp_file:
        # ì—…ë¡œë“œëœ íŒŒì¼ì˜ ë‚´ìš©ì„ ì„ì‹œ íŒŒì¼ì— ê¸°ë¡
        tmp_file.write(_file.getvalue())
        
        # ì„ì‹œ íŒŒì¼ì˜ ê²½ë¡œë¥¼ ë³€ìˆ˜ì— ì €ì¥
        tmp_file_path = tmp_file.name
        
        # ì„ì‹œ íŒŒì¼ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  í˜ì´ì§€ë¥¼ ë¶„í• 
        loader = PyPDFLoader(file_path=tmp_file_path)
        pages = loader.load_and_split()
        
        return pages # ë¶„í• ëœ í˜ì´ì§€ë“¤ì„ ë°˜í™˜
    
# í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ Chroma(VectorDB) ì•ˆì— ì„ë² ë”© ë²¡í„°ë¡œ ì €ì¥
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, separators=["\n\n", "\n", " ", ""])
    split_docs = text_splitter.split_documents(_docs)
    persist_directory = "./chroma_db"
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=persist_directory
    )
    return vectorstore

# ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ëŠ” í—¬í¼ í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
    

# PDF ë¬¸ì„œê¸°ë°˜ RAG ì²´ì¸ êµ¬ì¶•
@st.cache_resource
def chaining(_pages):
    vectorstore = create_vector_store(_pages)
    retriever = vectorstore.as_retriever()
    
    qa_system_prompt = """
        ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤. 
        ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
        ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
        í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
        ê°„ëµí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

        ## ë‹µë³€ ì˜ˆì‹œ
        **[ë‹µë³€]**
        ë‹µë³€ ë‚´ìš© ì„œìˆ 
        \n
        **[ë¬¸ì„œ ë‚´ ì¶œì²˜ ìœ„ì¹˜]**
        ë¬¸ì„œ ë‚´ ì¶œì²˜ í˜ì´ì§€ì™€ ì •ë³´ ìœ„ì¹˜ ì„œìˆ 

        {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )
    
    llm = ChatOpenAI(model="gpt-4o-mini")
    
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    
    return rag_chain


# Streamlit UI
st.title("PDF ì±—ë´‡ ğŸ’¬")

uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.", type=["pdf"])
if uploaded_file is not None:
    pages = load_pdf(uploaded_file)

    rag_chain = chaining(pages)

    # session_stateì— messages Key ê°’ ì§€ì • ë° Streamlit í™”ë©´ ì§„ì… ì‹œ, AIì˜ ì¸ì‚¬ë§ì„ ê¸°ë¡í•˜ê¸°
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {
                "role": "assistant",
                "content": "ì•ˆë…•í•˜ì„¸ìš”. ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"            
            }
        ]
    
    # ì‚¬ìš©ìë‚˜ AIê°€ ì§ˆë¬¸/ë‹µë³€ ì£¼ê³  ë°›ì„ ì‹œ, ì´ë¥¼ ê¸°ë¡í•˜ëŠ” session_state
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :"):
        st.chat_message("human").write(prompt_message)
        st.session_state.messages.append({"role": "user", "content": prompt_message})
        with st.chat_message("ai"):
            with st.spinner("ë‘ë‡Œ í’€ê°€ë™ ì¤‘..."):
                response = rag_chain.invoke(prompt_message)
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.write(response)